---
title: |
  | Name: Christopher Allwood
  | Student ID: 3174698
  |
  |
  | ECON 7201
  | Applied Econometrics
subtitle: "Assignment 2"
format: 
  pdf:
    include-in-header: 
      text: |
        \usepackage{fancyhdr}
        \fancypagestyle{style2}{
        \fancyhf{}
        \fancyhead[R]{Assignment 1}
        \fancyhead[L]{ECON 3201}
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{1pt}
        }
        \pagestyle{style2}
execute: 
  eval: true
  echo: true
---

```{r}
#| label: setup 
#| include: FALSE

#install.packages("tikzDevice")
# Load the tikzDevice library
library(tikzDevice)

# Set the default graphics device to tikz for the entire document
knitr::opts_chunk$set(dev = 'tikz', fig.width = 6, fig.height = 4)

set.seed(42)
```

\vspace{-1in}

## Due Date

**Sunday October 5, 2025** at 11:59 PM

## Directions

Answer all questions. Submit both a PDF and Quarto file to the nexus assignment portal.  

# Git and GitHub

1. 
    (a) Create a new R project in your **econ_3201** directory called **assignment_2**.
    (b) Download the assignment PDF and Quarto file the **assignment_2** folder.
    (c) Commit and push the changes to your **econ_3201** repository on [GitHub.com](GitHub.com).


# LaTeX

Matrices are created in LaTeX using the `\begin{bmatrix}...\end{bmatrix}` command. To separate entries along the same row, use `&`. To end a line, use `\\`. To make vertical elipses ($\vdots$), use `\vdots`. Practice writing the following matrices and vectors in LaTeX. Write the following matrices in LaTeX.

2.
    (a) 
$$
X'X = 
          \begin{bmatrix}
          n & \sum_{i=1}^n x_{1i} & \sum_{i=1}^n x_{2i} \\
          \sum_{i=1}^n x_{1i} & \sum_{i=1}^n x_{1i}^2 & \sum_{i=1}^n x_{1i}x_{2i}\\
          \sum_{i=1}^n x_{2i} & \sum_{i=1}^n x_{1i}x_{2i} & \sum_{i=1}^n x_{2i}^2
          \end{bmatrix}
$$
    

    (b) 
$$
          \Omega =
          \begin{bmatrix}
          \sigma_{1}^2 & 0 & 0 & 0 \\
          0 & \sigma_{2}^2 & 0 & 0 \\
          0 & 0 & \sigma_{3}^2 & 0 \\
          0 & 0 & 0 & \sigma_{4}^2 \\
          \end{bmatrix}
$$

# R

3. In this question we compare standard errors based on (incorrect) asymptotic assumptions with those based on alternate (appropriate) estimator (White). Consider one sample drawn from the following data generating process
(DGP) which we will simulate in `R`:

```{r}
#| eval: true
#| echo: true

set.seed(123)
n <- 25
x <- rnorm(n,mean=0.0,sd=1.0)
beta0 <- 1
beta1 <- 0
## x is irrelevant in this model, the data generating process is as follows:
dgp <- beta0 + beta1*x
## The residual is heteroskedastic by construction
e <- x^2*rnorm(n,mean=0.0,sd=1.0)
y <- dgp + e
```

> (a) Compute the OLS estimator of $\beta_2$ and its standard error using the `lm()` command in `R` for the model $y_i=\beta_1+\beta_2 x_i+\epsilon_i$ based on the DGP given above.

```{r}
OLS_Estimator <-lm(y~x)

summary(OLS_Estimator)
```



> (b) Next, compute the standard error of $\hat\beta_2$ by computing $\hat\sigma^2(X'X)^{-1}$ in `R` using matrix commands, and verify that the two standard error estimates are identical.

```{r}
X <- cbind(1, x)
residuals <-resid(OLS_Estimator)
sigmasqr_hat <-sum(residuals^2/(n-2))
trasposeX.Xinv <-solve(t(X) %*% X)
VarianceMatrix <-sigmasqr_hat * trasposeX.Xinv
beta2_se <-sqrt(VarianceMatrix[2,2])

#Comparing the result of beta_se with the standard error of x from the OLS_Estimates regression output.

beta2_se
coef(summary(OLS_Estimator))["x", "Std. Error"]


cbind(matrix= as.numeric(beta2_se), coef(summary(OLS_Estimator))["x", "Std. Error"])
```



> (c) Compute White's heteroskedasticity consistent covariance matrix estimator using matrices in R and report the White estimator of the standard error of $\hat\beta_2$. Compare this with that from 3 (a) above.

```{r}

White_X <- cbind(1,x)
transposeX.Xinv <-solve(t(X)%*%X)

#Extracting the residuals from the original regression output

resid_extract <-resid(OLS_Estimator)

# Transforming the residuals
resid_extract_hat <-diag(resid_extract^2)

White_resid <- t(White_X)%*% resid_extract_hat %*% White_X

CovWhite <- transposeX.Xinv %*% White_resid %*% transposeX.Xinv

# Extracting the standard corrected standard errors from 

White_SE_Beta2 <- sqrt(CovWhite[2,2])

# Comparing results

White_SE_Beta2
coef(summary(OLS_Estimator))["x", "Std. Error"]
cbind(White_SE_Beta2,coef(summary(OLS_Estimator))["x", "Std. Error"])


```



&nbsp;

4. Let $\hat{\theta}$ be an estimator for the population parameter $\theta$. $\hat{\theta}$ is said to be unbiased if $E(\hat\theta)=\theta$. That is, if the mean of the sampling distribution of $\hat{\theta}$ is equal to the true population value. \
\
Consider the model $$y_i=\beta_0+\beta_1x_{1,i}-\beta_2x_{2,i}+\epsilon_i.$$ Lets provide empirical evidence that the ordinary least squares estimators $\hat\beta_0$, $\hat\beta_1$, and $\hat\beta_2$ are unbiased estimators of $\beta_0$, $\beta_1$, $\beta_2$, respectively, using R.  
    (a) Set the seed to 1, i.e., `set.seed(1)`.
    
```{r}
#rm(list=ls())
set.seed(1)
```
    
    
    (b) Set the number of observations $n=100$
    
```{r}
n=(100)
```
    
    
    (c) Generate the following model $$y_i=2+3.5x_{1,i}-9.2x_{2,i}+\epsilon_i,$$ where $x_1\sim N(3,6)$, $x_2\sim N(2,4)$, and $\epsilon\sim N(0,100)$. To create a normally distributed variable, use the `rnorm(n, mean, sd)` command in R. 
    
```{r}
x_1 <- rnorm(n,mean=3,sd=sqrt(6))
x_2 <-rnorm(n,mean=2,sd=sqrt(4))
epsilon <- rnorm(n,mean=0,sd=sqrt(100))
y <-2+3.5*x_1-9.2*x_2+epsilon
model4c <- lm(y ~x_1+x_2)
summary(model4c)

```
    
    
    (d) Estimate the model coefficients using the `lm()` command. (Search `?lm()` in the console for more info).
    
```{r}
lm(y~x_1+x_2)$coefficients
```
    
    
    (e) Using a `for()` loop, replicate the model above $M=1000$ times and save the coefficient estimates from each iteration. 
    
```{r}
set.seed(1)
n=100
M=1000

estimates_hat <- matrix( , nrow=M, ncol=3)

for (i in 1:M){
  x_1hat <- rnorm(n,mean=3,sd=sqrt(6))
  x_2hat <-rnorm(n,mean=2,sd=sqrt(4))
  epsilonhat <- rnorm(n,mean=0,sd=sqrt(100))
  yhat <-2+3.5*x_1hat-9.2*x_2hat+epsilonhat
  model4c_hat <- lm(yhat ~x_1hat+x_2hat)
  summary(model4c_hat)
  estimates_hat[i, ]<-coef(model4c_hat)
}

summary(model4c_hat)
colnames(estimates_hat) <-c("Intercept","Beta1_hat","Beta2_hat" )
lm(yhat~x_1hat+x_2hat)$coefficients
```
    
    
    (f) Using `hist()`, plot the sampling distributions of the coefficient estimates, $\beta_1$ and $\beta_2$. 
    
```{r}
#| eval: true
#| echo: true
#| fig-cap: $"Sampling distribution from 1000 iterations for \hat{\beta_1}"$


hist(estimates_hat[, "Beta1_hat"],
     main="Sampling Distribution from for Beta-1-hat from 1000 iterations",
     xlab = "Beta-1-hat",
     col= "blue")
```
 
 
```{r}
#| eval: true
#| echo: true
#| fig-cap: $"Sampling distribution from 1000 iterations for \hat{\beta_2}"$

hist(estimates_hat[, "Beta2_hat"],
     main="Sampling Distribution from for Beta-2-hat from 1000 iterations",
     xlab = "Beta-2-hat",
     col= "green")
```
    
    
    (g) Add a vertical line to each figure at the mean of the respective variable. Search `?abline()` in your console.
    
    
```{r}
#| eval: true
#| echo: true
#| fig-cap: $"Sampling distribution from 1000 iterations for \hat{\beta_1} highlighting the mean of the distribution"$

hist(estimates_hat[ , "Beta1_hat"],
     main= "Sampling Distribution from for Beta-1-hat from 1000 iterations",
     xlab = "Beta-1-hat",
     col= "blue")


abline(v= mean(estimates_hat[, "Beta1_hat"]), 
       col= "red", 
       lwd=2, 
       lty="dashed")
```
```{r}
#| eval: true
#| echo: true
#| fig-cap: $"Sampling distribution from 1000 iterations for \hat{\beta_2} highlighting the mean  of the distribution"$

hist(estimates_hat[ , "Beta2_hat"],
     main="Sampling Distribution from for Beta-2-hat from 1000 iterations",
     xlab = "Beta-2-hat",
     col= "green")

abline(v= mean(estimates_hat[, "Beta2_hat"]), 
       col= "red", 
       lwd=2, 
       lty= "dashed")
```
    

